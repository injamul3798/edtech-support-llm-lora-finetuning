# -*- coding: utf-8 -*-
"""fine_tuning_llm_with_lora_deepseek_r1_distill_qwen_1_5b.ipynb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_3MXyG1tUxkw0OTb4E_I7YlIjIthcof5
"""

print('its working')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/LLM-training

!pip -q install -U "transformers>=4.41" "datasets>=2.19" "accelerate>=0.30" "trl>=0.9" "peft>=0.11" bitsandbytes
print('success')

!pip install --upgrade --force-reinstall pyarrow

import pandas as pd
from datasets import Dataset, DatasetDict
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

CSV_PATH = "/content/LLM-training/dataset/edtech_sft_dataset_100_with_company_info.csv"
df = pd.read_csv(CSV_PATH)

# Preprocessing
df["input"]  = df["input"].astype(str).str.replace("â€™", "’").str.replace("â€“", "–").str.replace("â€œ", "“").str.replace("â€˜", "‘").str.replace("â€", "”")
df["output"] = df["output"].astype(str).str.replace("â€™", "’").str.replace("â€“", "–").str.replace("â€œ", "“").str.replace("â€˜", "‘").str.replace("â€", "”")

# Since the dataset is small, we used the entire dataset for training to improve the model’s performance.
full_dataset = Dataset.from_pandas(df)

# Create 20% validation set FROM the same dataset
eval_dataset = full_dataset.shuffle(seed=42).select(
    range(int(0.2 * len(full_dataset)))
)

# Train uses ALL data
ds = DatasetDict({
    "train": full_dataset,   # 100%
    "test": eval_dataset     # 20% subset of the same data
})

print("Total samples:", len(full_dataset))
print("Train samples (100%):", len(ds["train"]))
print("Validation samples (20%):", len(ds["test"]))

SYSTEM = "You are a helpful customer support assistant for an ed-tech platform. Answer clearly and politely."

def format_example(ex):
    # Keep it simple + consistent (works for any base causal LM)
    return {
        "text": f"{SYSTEM}\n\nUser: {ex['input']}\nAssistant: {ex['output']}"
    }

ds = ds.map(format_example, remove_columns=ds["train"].column_names)
ds

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

tokenizer = AutoTokenizer.from_pretrained(model_name)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)



from peft import LoraConfig

peft_config = LoraConfig(
    r=6,                    # small rank (dataset is small)
    lora_alpha=8,           # ~2x rank
    lora_dropout=0.05,
    bias="none",
    target_modules="all-linear",
    task_type="CAUSAL_LM",
)

from trl import SFTConfig

args = SFTConfig(
    output_dir="./lora_adapter_out",
    max_steps=300,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    logging_steps=10,
    save_steps=100,
    eval_strategy="steps",
    eval_steps=50,
    fp16=True,
    bf16=False,
)

from trl import SFTTrainer

trainer = SFTTrainer(
    model=model,
    args=args,
    train_dataset=ds["train"],
    eval_dataset=ds["test"],
    peft_config=peft_config,
    processing_class=tokenizer,
)

trainer.train()

trainer.save_model("./lora_adapter_out")
tokenizer.save_pretrained("./lora_adapter_out")

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

base_model_name = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

tokenizer = AutoTokenizer.from_pretrained("./lora_adapter_out")

base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

model = PeftModel.from_pretrained(
    base_model,
    "./lora_adapter_out",
    torch_dtype=torch.float16
)

model.eval()

SYSTEM = "You are a helpful customer support assistant for an ed-tech platform. Answer clearly and politely."

print("Chat started. Type 'exit' to stop.\n")

while True:
    user_input = input("User: ")
    if user_input.lower() in ["exit", "quit"]:
        break

    prompt = f"""{SYSTEM}

User: {user_input}
Assistant:"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=120,
            temperature=0.3,
            top_p=0.9,
            do_sample=True
        )

    response = tokenizer.decode(output[0], skip_special_tokens=True)
    print("\nAssistant:", response.split("Assistant:")[-1].strip(), "\n")

SYSTEM = "You are a helpful customer support assistant for an ed-tech platform. Answer clearly and politely."

print("Chat started. Type 'exit' to stop.\n")

while True:
    user_input = input("User: ")
    if user_input.lower() in ["exit", "quit"]:
        break

    prompt = f"""{SYSTEM}

User: {user_input}
Assistant:"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        output = model.generate(
            **inputs,
            max_new_tokens=120,
            temperature=0.3,
            top_p=0.9,
            do_sample=True
        )

    response = tokenizer.decode(output[0], skip_special_tokens=True)
    print("\nAssistant:", response.split("Assistant:")[-1].strip(), "\n")



BASE_MODEL = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
ADAPTER_DIR = "./lora_adapter_out"
MERGED_DIR = "./merged_model"

# Load tokenizer (from adapter path)
tokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR)

# Load base model
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float16,
    device_map="auto"
)

# Load LoRA adapter
peft_model = PeftModel.from_pretrained(
    base_model,
    ADAPTER_DIR,
    torch_dtype=torch.float16
)

# Merge adapter weights into base model
merged_model = peft_model.merge_and_unload()

# Save merged model + tokenizer
merged_model.save_pretrained(MERGED_DIR)
tokenizer.save_pretrained(MERGED_DIR)

print("LoRA adapter successfully merged and saved to:", MERGED_DIR)